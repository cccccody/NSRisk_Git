{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import date\n",
    "from yahooquery import Ticker\n",
    "from utils_VaR import plot_scatter, plot_time_series_histogram, Security\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" visualization \"\"\"\n",
    "# T: time horizon (in days) for computing risk metrics\n",
    "# confidence level: 1-p for VaR, ES \n",
    "T = 120; p=5\n",
    "\n",
    "t0='1920-01-01'\n",
    "t1=date.today()\n",
    "\n",
    "sp500 = Security('^GSPC')\n",
    "sp500.set_df_hist(t0=t0, t1=t1)\n",
    "sp500.set_df_pct_change()\n",
    "sp500.set_df_risk(T=T, p=p, rolling_T=1) # to generate plot, set rolling T to 1. re-sampling is done in plot function for now\n",
    "# read start and end date of past US recessions\n",
    "recession_periods = pd.read_csv('../data/recession_periods_NBER.csv') # use dates classified by NBER\n",
    "# generate high_vol_periods\n",
    "sp500.label_vol_level_df(T=T)\n",
    "df = sp500.get_df_risk()\n",
    "df = df[df['vol'] > df['vol'].median()]\n",
    "high_vol_periods = df[::-1][::T][::-1][['start', 'end']]\n",
    "high_vol_periods.columns = ['Peak', 'Trough']\n",
    "\n",
    "# scatter plot with mask\n",
    "# plot_scatter(sp500.get_df_risk(), t0='1920-1-1', t1='2020-6-30', T=1, marked_periods=high_vol_periods)\n",
    "# times series histogram\n",
    "# plot_time_series_histogram(sp500.get_df_risk(), t0='2005-1-1', t1='2010-6-30', T=T, marked_periods=recession_periods, date_format='%B-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(sp500.get_df_risk(), t0='1920-1-1', t1='2020-6-30', T=1, marked_periods=recession_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_series_histogram(sp500.get_df_risk(), t0='2005-1-1', t1='2010-6-30', T=T, marked_periods=recession_periods, date_format='%B-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from utils_VaR import plot_scatter, plot_time_series_histogram, Security\n",
    "from scipy.stats import ttest_ind\n",
    "from datetime import date\n",
    "\n",
    "\"\"\" collect analytic results for different assets \"\"\"\n",
    "# read recession periods\n",
    "recession_periods = pd.read_csv('../data/recession_periods_NBER.csv') # use dates classified by NBER\n",
    "recession_periods = recession_periods.astype({'Peak':'datetime64', 'Trough':'datetime64'})\n",
    "peaks = recession_periods['Peak'].values\n",
    "troughs = recession_periods['Trough'].values\n",
    "\n",
    "# set asset ticker and set time horizon and p level for computing VaR, ES\n",
    "securities = dict(sp500='^GSPC', agg='agg', gold='XAU=', oil='CLc1', GBPUSD='GBP=', tech_sec='.IXTTR', energy_sec='.IXE', russell_2k='.RUT') \n",
    "dict_Ts = dict(sp500=100, agg=100, gold=50, oil=50, GBPUSD=50, tech_sec=50, energy_sec=50, russell_2k=50)\n",
    "\n",
    "# securities = dict(sp500='^GSPC', gold='XAU=') \n",
    "# dict_Ts = dict(sp500=100, gold=100)\n",
    "\n",
    "p=5\n",
    "# overall time horizon we want to consider\n",
    "t0 = '1920-1-1'\n",
    "t1 = date.today()\n",
    "\n",
    "# compute VaR, ES for all securities\n",
    "for i, key in enumerate(securities):\n",
    "    # length of time period for computing VaR, ES\n",
    "    T = dict_Ts[key]\n",
    "    rolling_T = T\n",
    "    # get historical data, compute VaR, ES\n",
    "    securities[key] = Security(securities[key]) # input ticker\n",
    "    securities[key].set_df_hist(t0=t0, t1=t1)\n",
    "    securities[key].set_df_pct_change()\n",
    "    securities[key].set_df_risk(T=dict_Ts[key], p=p, rolling_T=rolling_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" by recession \"\"\"\n",
    "columns=['asset', 'VaR', 'ES', 'ES/VaR', 'recession','sample size', 't-statistic', 'p-value']\n",
    "output = {}\n",
    "for i, key in enumerate(securities):\n",
    "    securities[key].label_recession_df(peaks=peaks, troughs=troughs, df='risk') # add recession (True/False) column\n",
    "    df = securities[key].get_df_risk()\n",
    "    \n",
    "    r = df[df['recession'] == True][['VaR', 'ES']] # all VaR and ES in recession periods\n",
    "    r_VaR_to_ESs = (r['ES']/r['VaR']).values # all VaR to ES ratios in recession periods\n",
    "    nr = df[df['recession'] == False][['VaR', 'ES']]\n",
    "    nr_VaR_to_ESs = (nr['ES']/nr['VaR']).values\n",
    "\n",
    "    ttest_res = ttest_ind(a=r_VaR_to_ESs, b=nr_VaR_to_ESs, equal_var=False)\n",
    "\n",
    "    output[i*2] = [key, np.mean(nr['VaR'].values), np.mean(nr['ES'].values), np.mean(nr_VaR_to_ESs), 'F', len(nr), '', '']\n",
    "    output[i*2+1] = [key, np.mean(r['VaR'].values), np.mean(r['ES'].values), np.mean(r_VaR_to_ESs), 'T', len(r), ttest_res[0], ttest_res[1]]\n",
    "\n",
    "df_tally = pd.DataFrame.from_dict(output, orient='index', columns=columns)\n",
    "df_tally.to_csv(f'ES_VaR_Ratio_by_recession_{t0}-{t1}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" by market vol levels \"\"\"\n",
    "columns=['asset', 'VaR', 'ES', 'ES/VaR', 'volatility level','sample size', 't-statistic', 'p-value']\n",
    "output = {}\n",
    "for i, key in enumerate(securities):\n",
    "\n",
    "    securities[key].label_vol_level_df(T=dict_Ts[key]) # add vol level column\n",
    "\n",
    "    df = securities[key].get_df_risk()\n",
    "    \n",
    "    low = df[df['vol level'] == 'low'][['VaR', 'ES']] \n",
    "    low_VaR_to_ESs = (low['ES']/low['VaR']).values \n",
    "    high = df[df['vol level'] == 'high'][['VaR', 'ES']]\n",
    "    high_VaR_to_ESs = (high['ES']/high['VaR']).values\n",
    "\n",
    "    ttest_res = ttest_ind(a=low_VaR_to_ESs, b=high_VaR_to_ESs, equal_var=False)\n",
    "\n",
    "    output[i*2] = [key, np.mean(low['VaR'].values), np.mean(low['ES'].values), np.mean(low_VaR_to_ESs), 'Low', len(low), '', '']\n",
    "    output[i*2+1] = [key, np.mean(high['VaR'].values), np.mean(high['ES'].values), np.mean(high_VaR_to_ESs), 'High', len(high), ttest_res[0], ttest_res[1]]\n",
    "\n",
    "df_tally = pd.DataFrame.from_dict(output, orient='index', columns=columns)\n",
    "df_tally.to_csv(f'ES_VaR_Ratio_by_realized_vol_{t0}-{t1}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking non-overlapping time periods leaves us with fewer data points but it's pretty much in keeping with the assumption that\n",
    "two samples are independent when doing a t-test. However, we are left with small sample size (<30), so that the mean of a sample should follow a normal distribution may not hold. \n",
    "\n",
    "Taking over-lapping time periods significant violated t-test assumption due to strong auto-correlations. Currently, historical prices for all securities except sp500 are only available from 2000 to today, on Yahoo finance. For some securities, such as GBPUSD, more data seems to be available elsewhere, which can be 'manually' fed into the program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Generate Results for Random LS Portfolios on sp500 constituents \"\"\"\n",
    "\n",
    "from datetime import date\n",
    "from utils_VaR import Security\n",
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "t0 = pd.to_datetime('1985-1-1')\n",
    "t1 = date.today()\n",
    "\n",
    "# sp500_const = pd.read_csv('../data/sp500_const.csv', parse_dates=[0], index_col=0).loc[t0:].dropna(axis='columns')\n",
    "# tickers = sp500_const.columns\n",
    "# remove ones that were not included in sp500 at the time\n",
    "df = pd.read_csv('../data/sp500_historical_constituents.csv').astype({'from':'datetime64'})\n",
    "elig_tickers = df[(df['from']>t0) & (df['thru'].isnull())]['co_tic'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "BRK.B\n"
    }
   ],
   "source": [
    "from yahooquery import Ticker\n",
    "date_index = pd.to_datetime(pd.read_csv('../data/business_date.csv', header=None)[0].values)\n",
    "sp500_const_new = pd.DataFrame(index=date_index)\n",
    "for ticker in elig_tickers:\n",
    "    try:\n",
    "        series = Ticker(ticker).history(period='max')['adjclose']\n",
    "        series.index = series.index.normalize()\n",
    "        sp500_const_new[ticker] = series\n",
    "    except:\n",
    "        print(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elig_tickers = list(map(str,elig_tickers))\n",
    "# Ticker('APD').history(period='max')['adjclose']\n",
    "sp500_const = sp500_const_new.loc[t0:].dropna(axis='columns')\n",
    "tickers = sp500_const.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "num_iter = 2000\n",
    "portfolios = {}\n",
    "T = 100\n",
    "p = 5\n",
    "num_const =20\n",
    "average_pct_change = np.array([0.]*(len(sp500_const)-1))\n",
    "for i in range(num_iter):\n",
    "    sampled_tickers = [tickers[i] for i in np.random.choice(len(tickers), num_const)] # sample from tickers universe\n",
    "    weights = np.random.permutation([0.1]*10+[-0.1]*10)\n",
    "\n",
    "    # computes portfolio pct_change\n",
    "    pct_change = sp500_const[sampled_tickers].pct_change().dropna().values @ weights\n",
    "    average_pct_change += pct_change\n",
    "    portfolio_df = pd.DataFrame(index=sp500_const.index[1:], data=pct_change)\n",
    "\n",
    "    # save result from each iteration\n",
    "    portfolios[i] = Security(i)\n",
    "    portfolios[i].set_df_pct_change_local(t0=t0, t1=t1, df=portfolio_df)\n",
    "    portfolios[i].set_df_risk(T, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        VaR        ES    ES/VaR recession  sample size t-statistics  \\\n0  0.012094  0.016712  1.385134         F       156000                \n1  0.017078  0.023319  1.370453         T        22000     -9.13155   \n\n       p-value  \n0               \n1  1.15931e-19  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VaR</th>\n      <th>ES</th>\n      <th>ES/VaR</th>\n      <th>recession</th>\n      <th>sample size</th>\n      <th>t-statistics</th>\n      <th>p-value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.012094</td>\n      <td>0.016712</td>\n      <td>1.385134</td>\n      <td>F</td>\n      <td>156000</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.017078</td>\n      <td>0.023319</td>\n      <td>1.370453</td>\n      <td>T</td>\n      <td>22000</td>\n      <td>-9.13155</td>\n      <td>1.15931e-19</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "\"\"\" by recession \"\"\"\n",
    "\n",
    "# read recession periods\n",
    "recession_periods = pd.read_csv('../data/recession_periods_NBER.csv') # use dates classified by NBER\n",
    "recession_periods = recession_periods.astype({'Peak':'datetime64', 'Trough':'datetime64'})\n",
    "peaks = recession_periods['Peak'].values\n",
    "troughs = recession_periods['Trough'].values\n",
    "\n",
    "columns=['asset', 'VaR', 'ES', 'ES/VaR', 'recession']\n",
    "output = {}\n",
    "for i, key in enumerate(portfolios):\n",
    "    portfolios[key].label_recession_df(peaks=peaks, troughs=troughs, df='risk') # add recession (True/False) column\n",
    "    df = portfolios[key].get_df_risk()\n",
    "    \n",
    "    r = df[df['recession'] == True][['VaR', 'ES']] # all VaR and ES in recession periods\n",
    "    r_VaR_to_ESs = (r['ES']/r['VaR']).values # all VaR to ES ratios in recession periods\n",
    "    nr = df[df['recession'] == False][['VaR', 'ES']]\n",
    "    nr_VaR_to_ESs = (nr['ES']/nr['VaR']).values\n",
    "\n",
    "    # ttest_res = ttest_ind(a=r_VaR_to_ESs, b=nr_VaR_to_ESs, equal_var=False)\n",
    "\n",
    "    output[i*2] = [key, np.mean(nr['VaR'].values), np.mean(nr['ES'].values), np.mean(nr_VaR_to_ESs), 'F']\n",
    "    output[i*2+1] = [key, np.mean(r['VaR'].values), np.mean(r['ES'].values), np.mean(r_VaR_to_ESs), 'T']\n",
    "\n",
    "\n",
    "temp= pd.DataFrame.from_dict(output, orient='index', columns=columns)\n",
    "\n",
    "nr_VaR_bar = np.mean(temp[temp['recession'] == 'F']['VaR'].values)\n",
    "r_VaR_bar = np.mean(temp[temp['recession'] == 'T']['VaR'].values)\n",
    "nr_ES_bar = np.mean(temp[temp['recession'] == 'F']['ES'].values)\n",
    "r_ES_bar = np.mean(temp[temp['recession'] == 'T']['ES'].values)\n",
    "nr_ratios = temp[temp['recession'] == 'F']['ES/VaR'].values\n",
    "r_ratios = temp[temp['recession'] == 'T']['ES/VaR'].values\n",
    "\n",
    "ttest_res = ttest_ind(a=r_ratios, b=nr_ratios, equal_var=False)\n",
    "\n",
    "pd.DataFrame.from_dict({0: [nr_VaR_bar, nr_ES_bar, np.mean(nr_ratios), 'F', len(nr)*num_iter,'', ''], 1:[r_VaR_bar, r_ES_bar, np.mean(r_ratios), 'T', len(r)*num_iter ,ttest_res[0], ttest_res[1]]} , columns=['VaR', 'ES', 'ES/VaR', 'recession','sample size', 't-statistics', 'p-value'], orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        VaR        ES    ES/VaR vol level  sample size t-statistics  \\\n0  0.009471  0.012794  1.362492       Low        90000                \n1  0.016022  0.022371  1.404620      High        88000     -28.6868   \n\n        p-value  \n0                \n1  1.73946e-160  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VaR</th>\n      <th>ES</th>\n      <th>ES/VaR</th>\n      <th>vol level</th>\n      <th>sample size</th>\n      <th>t-statistics</th>\n      <th>p-value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.009471</td>\n      <td>0.012794</td>\n      <td>1.362492</td>\n      <td>Low</td>\n      <td>90000</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.016022</td>\n      <td>0.022371</td>\n      <td>1.404620</td>\n      <td>High</td>\n      <td>88000</td>\n      <td>-28.6868</td>\n      <td>1.73946e-160</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "\"\"\" by market vol levels \"\"\"\n",
    "columns=['asset', 'VaR', 'ES', 'ES/VaR', 'vol level']\n",
    "output = {}\n",
    "for i, key in enumerate(portfolios):\n",
    "    portfolios[key].label_vol_level_df(T=T) # add vol level column\n",
    "    df = portfolios[key].get_df_risk()\n",
    "    \n",
    "    low = df[df['vol level'] == 'low'][['VaR', 'ES']] \n",
    "    low_VaR_to_ESs = (low['ES']/low['VaR']).values \n",
    "    high = df[df['vol level'] == 'high'][['VaR', 'ES']]\n",
    "    high_VaR_to_ESs = (high['ES']/high['VaR']).values\n",
    "\n",
    "    # ttest_res = ttest_ind(a=low_VaR_to_ESs, b=high_VaR_to_ESs, equal_var=False)\n",
    "\n",
    "    output[i*2] = [key, np.mean(low['VaR'].values), np.mean(low['ES'].values), np.mean(low_VaR_to_ESs), 'Low']\n",
    "    output[i*2+1] = [key, np.mean(high['VaR'].values), np.mean(high['ES'].values), np.mean(high_VaR_to_ESs), 'High']\n",
    "\n",
    "temp= pd.DataFrame.from_dict(output, orient='index', columns=columns)\n",
    "\n",
    "low_VaR_bar = np.mean(temp[temp['vol level'] == 'Low']['VaR'].values)\n",
    "high_VaR_bar = np.mean(temp[temp['vol level'] == 'High']['VaR'].values)\n",
    "low_ES_bar = np.mean(temp[temp['vol level'] == 'Low']['ES'].values)\n",
    "high_ES_bar = np.mean(temp[temp['vol level'] == 'High']['ES'].values)\n",
    "low_ratios = temp[temp['vol level'] == 'Low']['ES/VaR'].values\n",
    "high_ratios = temp[temp['vol level'] == 'High']['ES/VaR'].values\n",
    "\n",
    "ttest_res = ttest_ind(a=low_ratios, b=high_ratios, equal_var=False)\n",
    "\n",
    "pd.DataFrame.from_dict({0: [low_VaR_bar, low_ES_bar, np.mean(low_ratios), 'Low', len(low)*num_iter,'', ''], 1:[high_VaR_bar, high_ES_bar, np.mean(high_ratios), 'High', len(high)*num_iter ,ttest_res[0], ttest_res[1]]} , columns=['VaR', 'ES', 'ES/VaR', 'vol level','sample size', 't-statistics', 'p-value'], orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" generate sharpe ratio for insanity check \"\"\"\n",
    "\n",
    "columns=['asset', 'annualized return', 'annualized std', 'Sharpe Ratio']\n",
    "output = {}\n",
    "\n",
    "for i, key in enumerate(portfolios):\n",
    "    df = portfolios[key].get_df_pct_change()\n",
    "    annual_return = (df+1).cumprod().values[-1][0] **(1/(t1.year-t0.year)) - 1\n",
    "    annual_std = df.std().values[0] * ((252)**0.5)\n",
    "\n",
    "    output[i] = [key, annual_return, annual_std, annual_return/annual_std]\n",
    "\n",
    "df_tally = pd.DataFrame.from_dict(output, orient='index', columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     display(df_tally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitbaseconda19daccaab569477d9dec399eeb89c3e8",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}